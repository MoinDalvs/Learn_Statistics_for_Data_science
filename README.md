# Learn_stats_for-Data_science

![inbox_221701_c1e7f0c6c305bffe3d62bd37d8783ed8_prob](https://user-images.githubusercontent.com/99672298/190671924-4c9b0dd4-243b-4828-9ade-f91a0555bb16.png)
![30 03 2022_15 52 11_REC](https://user-images.githubusercontent.com/99672298/190672178-6591a16f-0939-4262-9226-3bcfb40a7e7f.png)
![30 03 2022_15 52 32_REC](https://user-images.githubusercontent.com/99672298/190672197-76e232bd-1202-408c-a7b0-1bde68671d38.png)
![30 03 2022_15 53 01_REC](https://user-images.githubusercontent.com/99672298/190672213-c2e51ed4-1840-4fa5-b51e-6a4916968f4f.png)

## Statistics and its types

Statistics is broadly categorized into two types:

+ Descriptive Statistics
+ Inferential Statistics

![01 02 2022_22 16 38_REC](https://user-images.githubusercontent.com/99672298/190960580-a9d37c78-33e0-4092-bb0c-035ed724d6b7.png)

### Descriptive Statistics:
It utilized numerical and graphical methods to look for patterns in a dataset, to summarize the information revealed in a dataset, and to present the information in a convinient form that individuals can use to make decisions( Mean, Median, Mode, Variance, Standard deviation and charts ore Probability Distribution)

Basically, as part of descriptive Statistics, we measure the following:

+ Frequency: no. of times a data point occurs
+ Central tendency: the centrality of the data – mean, median, and mode
+ Dispersion: the spread of the data – range, variance, and standard deviation
+ The measure of position: percentiles and quantile ranks

### Inferential Statistics
In Inferential statistics,  utilizing sample data, to estimate decisions, predictions or other generalization by running Hypothesis testing to assess the assumptions made about the population parameters.

In simple terms, we interpret the meaning of the descriptive statistics by inferring them to the population. The main goal of inferential statistics is to make a conclusion about a population based off a sample of data from the population. 

For example, we are conducting a survey on the number of two-wheelers in a city. Assume the city has a total population of 5L people. So, we take a sample of 1000 people as it is impossible to run an analysis on entire population data.

From the survey conducted, it is found that 800 people out of 1000 (800 out of 1000 is 80%) are using two-wheelers. So, we can infer these results to the population and conclude that 4L people out of the 5L population are using two-wheelers.

### Data Types and Level of Measurement

There is structured and unstructured data. Then you have qualitative and quantitative data. Now let's explore two more data types (discrete and continuous) and help you understand the difference. 

![image](https://user-images.githubusercontent.com/99672298/190947841-6179d7b7-2799-4856-b4be-d5987abfbde0.png)

At a higher level, data is categorized into two types: Qualitative and Quantitative.

![25 02 2022_15 16 47_REC](https://user-images.githubusercontent.com/99672298/190960830-5d75e078-ee70-49ee-98f1-20a6ce630ffc.png)

Qualitative data is non-numerical. Some of the examples are eye colour, car brand, city, etc.

On the other hand, Quantitative data is numerical, and it is again divided into Continuous and Discrete data.

![25 02 2022_15 17 21_REC](https://user-images.githubusercontent.com/99672298/190960918-62283d9a-addc-41dc-85e1-806dcb215fcd.png)

### Importance of Qualitative and Quantitative Data
Qualitative types of data in research work around the characteristics of the retrieved information and helps understand customer behavior. This type of data in statistics helps run market analysis through genuine figures and create value out of service by implementing useful information. Qualitative types of data in statistics can drastically affect customer satisfaction if applied smartly.

On the other hand, the Quantitative data types of statistical data work with numerical values that can be measured, answering questions such as ‘how much’, ‘how many’, or ‘how many times’. Quantitative data types in statistics contain a precise numerical value. Therefore, they can help organizations use these figures to gauge improved and faulty figures and predict future trends.

### Can Ordinal and Discrete type overlap?
If you pay attention to this, you can give numbering to the ordinal classes, and then it should be called discrete type or ordinal? The truth is that it is still ordinal. The reason for this is that even if the numbering is done, it doesn’t convey the actual distances between the classes.

For instance, consider the grading system of a test. The respective grades can be A, B, C, D, E, and if we number them from starting then it would be 1,2,3,4,5. Now according to the numerical differences, the distance between E grade and D grade is the same as the distance between the D and C grade which is not very accurate as we all know that C grade is still acceptable as compared to E grade but the mid difference declares them as equal.

You can also apply the same technique to a survey form where user experience is recorded on a scale of very poor to very good. The differences between various classes are not clear therefore can’t be quantified directly. 

**Continuous data:** It can be represented in decimal format. Examples are height, weight, time, distance, etc.

![image](https://user-images.githubusercontent.com/99672298/190947983-633b685c-8c2b-4d2d-9e19-59ae88813b17.png)

Discrete data: It cannot be represented in decimal format. Examples are the number of laptops, number of students in a class.

Discrete data is again divided into Categorical and Count Data.

**Categorical data:** represent the type of data that can be divided into groups. Examples are age, sex, etc.

**Count data:** This data contains non-negative integers. Example: number of children a couple has.

![image](https://user-images.githubusercontent.com/99672298/190947605-87aca262-3b39-49bc-98d2-ea4e200c91fb.png)

### Level of Measurement
In statistics, the level of measurement is a classification that describes the relationship between the values of a variable.

![image](https://user-images.githubusercontent.com/99672298/190948985-801268e9-f19a-4aa0-845e-54fdce98a1fb.png)
![image](https://user-images.githubusercontent.com/99672298/190952524-d2c841eb-e176-4bec-8c2f-20941eef601b.png)

We have four fundamental levels of measurement. They are:

+ Nominal Scale
+ Ordinal Scale
+ Interval Scale
+ Ratio Scale

![01 02 2022_22 23 08_REC](https://user-images.githubusercontent.com/99672298/190960629-6490a598-e380-437f-a3ef-089da872f66f.png)
![image](https://user-images.githubusercontent.com/99672298/190980595-655ecb68-d1ea-441a-abf1-7f46625ad73f.png)
![image](https://user-images.githubusercontent.com/99672298/190980780-6374b5bd-44f8-4c75-ae5c-259638d1068e.png)

1. **Nominal Scale:**
These are the set of values that don’t possess a natural ordering. Let’s understand this with some examples.
This scale contains the least information since the data have names/labels only. It can be used for classification. We cannot perform mathematical operations on nominal data because there is no numerical value to the options (numbers associated with the names can only be used as tags).
The nominal scale uses categories, so finding the median makes no sense. You could put the items in alphabetical order but even then, the middle item would have no meaning as a median. However, a mode (the most frequent item in the set) is possible. For example, if you were to survey a group of random people and ask them what the most romantic city in the World is, Venice or Paris might be the most common response (the mode). It is not possible to state that ‘Red’ is greater than ‘Blue’. The gender of a person is another one where we can’t differentiate between male, female, or others. 

Nominal data types in statistics are not quantifiable and cannot be measured through numerical units. Nominal types of statistical data are valuable while conducting qualitative research as it extends freedom of opinion to subjects.

Example: Which country do you belong to? India, Japan, Korea.
+ Gender: Male, Female, Other.
+ Hair Color: Brown, Black, Blonde, Red, Other.
+ Type of living accommodation: House, Apartment, Trailer, Other.
+ Genotype: Bb, bb, BB, bB.
+ Religious preference: Buddhist, Mormon, Muslim, Jewish, Christian, Other.

2. **Ordinal Scale:** 

These types of values have a natural ordering while maintaining their class of values. If we consider the size of a clothing brand then we can easily sort them according to their name tag in the order of small < medium < large. The grading system while marking candidates in a test can also be considered as an ordinal data type where A+ is definitely better than B grade. 

These categories help us deciding which encoding strategy can be applied to which type of data. Data encoding for Qualitative data is important because machine learning models can’t handle these values directly and needed to be converted to numerical types as the models are mathematical in nature.

For nominal data type where there is no comparison among the categories, one-hot encoding can be applied which is similar to binary coding considering there are in less number and for the ordinal data type, label encoding can be applied which is a form of integer encoding.

![image](https://user-images.githubusercontent.com/99672298/190952490-07302572-4556-4393-b3cb-56789c4be669.png)

These scales are generally used to depict non-mathematical ideas such as frequency, satisfaction, happiness, a degree of pain, etc. It is quite straightforward to remember the implementation of this scale as ‘Ordinal’ sounds similar to ‘Order’, which is exactly the purpose of this scale.

Ordinal data is made up of ordinal variables. In other words, if you have a list that can be placed in “first, second, third…” order, you have ordinal data. It sounds simple, but there are a couple of elements that can be confusing:
+ You don’t have to have the exact words “first, second, third….” Instead, you can have different rating scales, like “Hot, hotter, hottest” or “Agree, strongly agree, disagree.”
+ You don’t know if the intervals between the values are equal. We know that a list of cardinal numbers like 1, 5, 10 have a set value between them (in this case, 5) but with ordinal data you just don’t know. For example, in a marathon you might have first, second and third place. But if you don’t know the exact finishing times, you don’t know what the interval between first and second, or second and third is.

### Discrete
The numerical values which fall under are integers or whole numbers are placed under this category. The number of speakers in the phone, cameras, cores in the processor, the number of sims supported all these are some of the examples of the discrete data type.

Discrete data types in statistics cannot be measured – it can only be counted as the objects included in discrete data have a fixed value. The value can be represented in decimal, but it has to be whole. Discrete data is often identified through charts, including bar charts, pie charts, and tally charts.

### Continuous
 The fractional numbers are considered as continuous values. These can take the form of the operating frequency of the processors, the android version of the phone, wifi frequency, temperature of the cores, and so on. 

Unlike discrete data types of data in research, with a whole and fixed value, continuous data can break down into smaller pieces and can take any value. For example, volatile values such as temperature and the weight of a human can be included in the continuous value. Continuous types of statistical data are represented using a graph that easily reflects value fluctuation by the highs and lows of the line through a certain period of time. 

3. **Interval Scale:**
It is a numerical scale. The Interval scale has more information than the nominal, ordinal scales. Along with the order, we know the difference between the two variables (interval indicates the distance between two entities).

These scales are effective as they open doors for the statistical analysis of provided data. Mean, median, or mode can be used to calculate the central tendency in this scale. The only drawback of this scale is that there no pre-decided starting point or a true zero value.

Interval scale contains all the properties of the ordinal scale, in addition to which, it offers a calculation of the difference between variables. The main characteristic of this scale is the equidistant difference between objects.  

For instance, consider a Celsius/Fahrenheit temperature scale –

+ 80 degrees is always higher than 50 degrees and the difference between these two temperatures is the same as the difference between 70 degrees and 40 degrees.
+ Also, the value of 0 is arbitrary because negative values of temperature do exist – which makes the Celsius/Fahrenheit temperature scale a classic example of an interval scale.
+ Interval scale is often chosen in research cases where the difference between variables is a mandate – which can’t be achieved using a nominal or ordinal scale. The Interval scale quantifies the difference between two variables whereas the other two scales are solely capable of associating qualitative values with variables.
The mean and median values in an ordinal scale can be evaluated, unlike the previous two scales.
+ In statistics, interval scale is frequently used as a numerical value can not only be assigned to variables but calculation on the basis of those values can also be carried out.

4. **Ratio Scale:**
The ratio scale has the most information about the data. Unlike the other three scales, the ratio scale can accommodate a true zero point. The ratio scale is simply said to be the combination of Nominal, Ordinal, and Intercal scales.

With the option of true zero, varied inferential, and descriptive analysis techniques can be applied to the variables. In addition to the fact that the ratio scale does everything that a nominal, ordinal, and interval scale can do, it can also establish the value of absolute zero. The best examples of ratio scales are weight and height. In market research, a ratio scale is used to calculate market share, annual sales, the price of an upcoming product, the number of consumers, etc.

+ Ratio scale provides the most detailed information as researchers and statisticians can calculate the central tendency using statistical techniques such as mean, median, mode, and methods such as geometric mean, the coefficient of variation, or harmonic mean can also be used on this scale.
+ Ratio scale accommodates the characteristic of three other variable measurement scales, i.e. labeling the variables, the significance of the order of variables, and a calculable difference between variables (which are usually equidistant).
+ Because of the existence of true zero value, the ratio scale doesn’t have negative values.
+ To decide when to use a ratio scale, the researcher must observe whether the variables have all the characteristics of an interval scale along with the presence of the absolute zero value.
+ Mean, mode and median can be calculated using the ratio scale.

![25 02 2022_15 17 50_REC](https://user-images.githubusercontent.com/99672298/190960855-3ea2e75b-8c8c-4aee-b0c7-330da0afae19.png)
![image](https://user-images.githubusercontent.com/99672298/190954184-33f4918c-920e-4c3c-a2ab-4013e6206fbc.png)
![image](https://user-images.githubusercontent.com/99672298/190954195-4019d825-3784-4328-a78f-1ab964bbbb93.png)
![image](https://user-images.githubusercontent.com/99672298/190954635-62b632cc-ef5d-4195-9760-8a4c1ffb5a4d.png)

## Moments of Business Decision

We have four moments of business decision that help us understand the data.

### Measures of Central tendency

(It is also known as First Moment Business Decision)

Talks about the centrality of the data. To keep it simple, it is a part of descriptive statistical analysis where a single value at the centre represents the entire dataset.

## Mean

It is the sum of all the data points divided by the total number of values in the data set. Mean cannot always be relied upon because it is influenced by outliers.

## MEdian

It is the middlemost value of a sorted/ordered dataset. If the size of the dataset is even, then the median is calculated by taking the average of the two middle values. In case of outliers Mean cannot be relied upon as much as median. A Median will have better representation of information when an outliers are present.

![15 09 2022_21 39 39_REC](https://user-images.githubusercontent.com/99672298/190981525-b87b46fc-3690-4822-a21e-a32f4bc90730.png)

## Mode

It is the most repeated value in the dataset. Data with a single mode is called unimodal, data with two modes is called bimodal, and data with more than two modes is called multimodal.

+ A multimodal distribution is a probability distribution with more than one peak, or “mode.”
+ A distribution with one peak is called unimodal
+ A distribution with two peaks is called bimodal
+ A distribution with two peaks or more is multimodal
+ A bimodal distribution is also multimodal, as there are multiple peaks.

+ A comb distribution is so-called because the distribution looks like a comb, with alternating high and low peaks. A comb shape can be caused by rounding off. For example, if you are measuring water height to the nearest 10 cm and your class width for the histogram is 5 cm, this could cause a comb shape.

![image](https://user-images.githubusercontent.com/99672298/190957391-7ee25a6e-7cb9-43dc-bc81-dac4102a570d.png)

An edge peak distribution is where there is an additional, out of place peak at the edge of the distribution. This usually means that you’ve plotted (or collected) your data incorrectly, unless you know for sure your data set has an expected set of outliers (i.e. a few extreme views on a survey).

![image](https://user-images.githubusercontent.com/99672298/190957418-32d1674c-97cb-49f8-88f5-aeefd22851d4.png)

A multimodal distribution is known as a Plateau Distribution when there are more than a few peaks close together.

![image](https://user-images.githubusercontent.com/99672298/190957449-01447354-8565-498f-aa9d-d42f10105e44.png)

#### Causes of a Multimodal Distribution
A multimodal distribution in a sample is usually an indication that the distribution in the population is not normal. It can also indicate that your sample has several patterns of response or extreme views, preferences or attitudes.

When thinking about the cause of the multimodality, you may want to take a close look at your data; what may be going on is that two or more distributions are being graphed at the same time. This is opposed to a true multimodal distribution, where only one distribution is mapped. For example, the following image shows two groups of students, one of which studied (the peak on the left) and one of which didn’t (the peak on the right).

![image](https://user-images.githubusercontent.com/99672298/190957530-3009f629-cf5c-436d-94ca-a7fe104dff6a.png)

<img width="620" alt="3739f8e23248b79e3002acc01005beb2" src="https://user-images.githubusercontent.com/99672298/190962957-3a75ea7f-2a4f-4b52-9db6-fb29666ce62c.png">

### Mean vs Median

Mean will tell the Average and in some sense it will give ust the central tendecy of the data. Median will give the middle most value of the data and in both case they are telling us central tendecy (centre of the data). In case of outliers mean cannot be trusted as much as median. A median will have better representation of information when as outliers are present and if the mean is greater than median the distribution is right skewed and if the mean is lesser than median the distribution is right skewed.

## The Difference Between Standard Deviation and Average Deviation

### Standard Deviation Versus Average Deviation
Two of the most popular ways to measure variability or volatility in a set of data are standard deviation and average deviation, also known as mean absolute deviation. Though the two measurements are similar, they are calculated differently and offer slightly different views of data.

Determining volatility—that is, deviation from the center—is important in finance, so professionals in accounting, investing, and economics should be familiar with both concepts.

#### KEY TAKEAWAYS
+ Standard deviation is the most common measure of variability and is frequently used to determine the volatility of financial instruments and investment returns.
+ Standard deviation is considered the most appropriate measure of variability when using a population sample, when the mean is the best measure of center, and when the distribution of data is normal.
+ Some argue that average deviation, or mean absolute deviation, is a better gauge of variability when there are distant outliers or the data is not well distributed.

### Understanding Standard Deviation
Standard deviation is the most common measure of variability and is frequently used to determine the volatility of markets, financial instruments, and investment returns. To calculate the standard deviation:

+ Find the mean, or average, of the data points by adding them and dividing the total by the number of data points.
+ Subtract the mean from each data point and square the difference of each result.
+ Find the mean of those squared differences and then the square root of the mean.

Squaring the differences between each point and the mean avoids the issue of negative differences for values below the mean, but it means the variance is no longer in the same unit of measure as the original data. Taking the square root means the standard deviation returns to the original unit of measure and is easier to interpret and use in further calculations.

### Average Deviation
The average deviation, or mean absolute deviation, is calculated similarly to standard deviation, but it uses absolute values instead of squares to circumvent the issue of negative differences between the data points and their means.

To calculate the average deviation:

+ Calculate the mean of all data points.
+ Calculate the difference between the mean and each data point.
+ Calculate the average of the absolute values of those differences.

### Standard Deviation Versus Average Deviation
Standard deviation is often used to measure the volatility of returns from investment funds or strategies because it can help measure volatility. Higher volatility is generally associated with a higher risk of losses, so investors want to see higher returns from funds that generate higher volatility. For example, a stock index fund should have relatively low standard deviation compared with a growth fund.

The mean average, or mean absolute deviation, is considered the closest alternative to standard deviation. It is also used to gauge volatility in markets and financial instruments, but it is used less frequently than standard deviation.

According to mathematicians, when a data set is of normal distribution—that is, there aren't many outliers—standard deviation is generally the preferable gauge of variability. But when there are large outliers, standard deviation registers higher levels of dispersion (or deviation from the center) than mean absolute deviation.


## Variance: It is the average squared distance of all the data points from their mean. The problem with Variance is, the units will also get squared.

![07 05 2022_13 13 53_REC](https://user-images.githubusercontent.com/99672298/190963012-a972ef22-0200-44d2-a4f3-8faa5efa176e.png)

## Standard Deviation: It is the square root of Variance. Helps in retrieving the original units.

## Range: It is the difference between the maximum and the minimum values of a dataset.

![15 09 2022_22 06 15_REC](https://user-images.githubusercontent.com/99672298/190981467-9fbdb171-665c-46bc-9bf2-62dc9b94459f.png)

## Skewness

![16 09 2022_19 29 02_REC](https://user-images.githubusercontent.com/99672298/190981085-24b6ee9a-e72a-4920-98bc-582ac2651fcb.png)

(It is also known as Third Moment Business Decision)

It measures the asymmetry in the data. The two types of Skewness are:

Positive/right-skewed: Data is said to be positively skewed if most of the data is concentrated to the left side and has a tail towards the right.

Negative/left-skewed: Data is said to be negatively skewed if most of the data is concentrated to the right side and has a tail towards the left.

![image](https://user-images.githubusercontent.com/99672298/190962416-8a01be82-e230-4e2e-be1c-da53288128eb.png)
![image](https://user-images.githubusercontent.com/99672298/190962452-0094c080-4269-430d-8c9c-5f2734979c22.png)
![sk1](https://user-images.githubusercontent.com/99672298/190672365-7fcd7401-b263-4a2e-916b-352663be040c.png)
![FRBfMkKaIAAjrd2](https://user-images.githubusercontent.com/99672298/190672080-c7c560e4-5447-4743-970d-839ee85898a0.png)
![26 02 2022_15 13 10_REC](https://user-images.githubusercontent.com/99672298/190962548-87f13d54-87b7-4edf-8c4e-d65a0e913657.png)

## Kurtosis

Kurtosis is a measure of the tailedness of a distribution. Tailedness is how often outliers occur. Excess kurtosis is the tailedness of a distribution relative to a normal distribution.
Like skewness, kurtosis is a statistical measure that is used to describe distribution. Whereas skewness differentiates extreme values in one versus the other tail, kurtosis measures extreme values in either tail. Distributions with large kurtosis exhibit tail data exceeding the tails of the normal distribution (e.g., five or more standard deviations from the mean). Distributions with low kurtosis exhibit tail data that are generally less extreme than the tails of the normal distribution.

In other words, Kurtosis is a statistical measure that defines how heavily the tails of a distribution differ from the tails of a normal distribution. In other words, kurtosis identifies whether the tails of a given distribution contain extreme values.

Along with skewness, kurtosis is an important descriptive statistic of data distribution. However, the two concepts must not be confused with each other. Skewness essentially measures the symmetry of the distribution, while kurtosis determines the heaviness of the distribution tails.

In finance, kurtosis is used as a measure of financial risk. A large kurtosis is associated with a high risk for an investment because it indicates high probabilities of extremely large and extremely small returns. On the other hand, a small kurtosis signals a moderate level of risk because the probabilities of extreme returns are relatively low.
For investors, high kurtosis of the return distribution implies the investor will experience occasional extreme returns (either positive or negative), more extreme than the usual + or - three standard deviations from the mean that is predicted by the normal distribution of returns. This phenomenon is known as kurtosis risk.

![image](https://user-images.githubusercontent.com/99672298/190942840-f4da242f-3086-4665-8bf9-70d3300d49ff.png)
![image](https://user-images.githubusercontent.com/99672298/190942616-ac87fa40-c453-4805-9a5d-c4965953979b.png)
![image](https://user-images.githubusercontent.com/99672298/190677717-f516eb9b-caed-4d49-95c4-60f6ea71290c.png)

### What is Excess Kurtosis?
Excess kurtosis is a metric that compares the kurtosis of a distribution against the kurtosis of a normal distribution. The kurtosis of a normal distribution equals 3. Therefore, the excess kurtosis is found using the formula below:

Excess Kurtosis = Kurtosis – 3

![image](https://user-images.githubusercontent.com/99672298/190942974-04437c8d-cd2b-41e9-adf0-b648a6b86c19.png)

Distributions with medium kurtosis (medium tails) are mesokurtic.
Distributions with low kurtosis (thin tails) are platykurtic.
Distributions with high kurtosis (fat tails) are leptokurtic.
Tails are the tapering ends on either side of a distribution. They represent the probability or frequency of values that are extremely high or low compared to the mean. In other words, tails represent how often outliers occur.

Example: Types of kurtosis

![image](https://user-images.githubusercontent.com/99672298/190672762-187cba1e-f8ed-4e98-a851-13b3af6f6f00.png)

Types of kurtosis
Distributions can be categorized into three groups based on their kurtosis:

![image](https://user-images.githubusercontent.com/99672298/190672814-2c8e38aa-38ef-459f-9158-3787b61143a5.png)
![image](https://user-images.githubusercontent.com/99672298/190672949-b7dca686-02f5-4037-b1bc-f4cb61f76a46.png)
![image](https://user-images.githubusercontent.com/99672298/190673076-13f3345f-f863-44d1-8d32-27fc5da83be1.png)

From the graph, we can see that the frequency distribution (shown by the gray bars) approximately follows a normal distribution (shown by the green curve). Normal distributions are mesokurtic.

The zoologist calculates the kurtosis of the sample. She finds that the kurtosis is 3.09 and the excess kurtosis is 0.09, and she concludes that the distribution is mesokurtic.

![image](https://user-images.githubusercontent.com/99672298/190673209-a780b0f0-f8c4-4988-a69c-0b202716988b.png)

### Leptokurtic
Leptokurtic indicates a positive excess kurtosis. The leptokurtic distribution shows heavy tails on either side, indicating large outliers. In finance, a leptokurtic distribution shows that the investment returns may be prone to extreme values on either side. Therefore, an investment whose returns follow a leptokurtic distribution is considered to be risky.

### Platykurtic
A platykurtic distribution shows a negative excess kurtosis. The kurtosis reveals a distribution with flat tails. The flat tails indicate the small outliers in a distribution. In the finance context, the platykurtic distribution of the investment returns is desirable for investors because there is a small probability that the investment would experience extreme returns.

## Percentile vs. Quartile vs. Quantile: What’s the Difference?

Three terms that students often confuse in statistics are percentiles, quartiles, and quantiles.

Here’s a simple definition of each:

Percentiles: Range from 0 to 100.

Quartiles: Range from 0 to 4.

+ Note that percentiles and quartiles share the following relationship:

+ 0 percentile = 0 quartile (also called the minimum)
+ 25th percentile = 1st quartile
+ 50th percentile = 2nd quartile (also called the median)
+ 75th percentile = 3rd quartile
+ 100th percentile = 4th quartile (also called the maximum)

Quantiles: Range from any value to any other value.

Note that percentiles and quartiles are simply types of quantiles.

![image](https://user-images.githubusercontent.com/99672298/190623416-d0621d79-121d-4ff2-8a98-083a2ec825ae.png)

Some types of quantiles even have specific names, including:

+ 4-quantiles are called quartiles.
+ 5-quantiles are called quintiles.
+ 8-quantiles are called octiles.
+ 10-quantiles are called deciles.
+ 100-quantiles are called percentiles.

![26 02 2022_13 27 12_REC](https://user-images.githubusercontent.com/99672298/190982569-a866dc6f-d50b-490a-953a-806c3789a9e3.png)

### What is a Quantile?
The word “quantile” comes from the word quantity. In simple terms, a quantile is where a sample is divided into equal-sized, adjacent, subgroups (that’s why it’s sometimes called a “fractile“). It can also refer to dividing a probability distribution into areas of equal probability.

The median is a quantile; the median is placed in a probability distribution so that exactly half of the data is lower than the median and half of the data is above the median. The median cuts a distribution into two equal areas and so it is sometimes called 2-quantile.

Quartiles are also quantiles; they divide the distribution into four equal parts. Percentiles are quantiles that divide a distribution into 100 equal parts and deciles are quantiles that divide a distribution into 10 equal parts

## Boxplot 

![26 02 2022_15 16 46_REC](https://user-images.githubusercontent.com/99672298/190982234-fd5a78fe-76e7-437c-bb31-8fcc2260bdb4.png)
![26 02 2022_15 15 12_REC](https://user-images.githubusercontent.com/99672298/190982264-09bf95b5-c1f8-49f1-ac66-e5e6d568dbe8.png)
![xBoxPlotandNormalDistribution JPG pagespeed ic 84HerCY3C7](https://user-images.githubusercontent.com/99672298/190981362-928d074f-e27c-4950-a21b-0172c862150e.jpg)
![xBoxPlot jpg pagespeed ic OUUzjm6fGL](https://user-images.githubusercontent.com/99672298/190981414-aa3df22c-2498-44d3-942c-8215bc2125b2.jpg)

## “Why 1.5 times IQR? Why not 1 or 2 or any other number?”

In the most general sense, an outlier is a data point which differs significantly from other observations.
### IQR Method of Outlier Detection
To explain IQR Method easily, let’s start with a box plot.

![image](https://user-images.githubusercontent.com/99672298/190669815-0ba7ed2e-9282-47c4-98bf-84c0600d0753.png)

A box plot tells us, more or less, about the distribution of the data. It gives a sense of how much the data is actually spread about, what’s its range, and about its skewness. As you might have noticed in the figure, that a box plot enables us to draw inference from it for an ordered data, i.e., it tells us about the various metrics of a data arranged in ascending order.

In the above figure,

+ minimum is the minimum value in the dataset,
+ and maximum is the maximum value in the dataset.

So the difference between the two tells us about the range of dataset.

+ The median is the median (or centre point), also called second quartile, of the data (resulting from the fact that the data is ordered).
+ Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.
+ Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3.
+ The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR.
### IQR = Q3 - Q1

To detect the outliers using this method, we define a new range, let’s call it decision range, and any data point lying outside this range is considered as outlier and is accordingly dealt with. The range is as given below:

### Lower Bound: (Q1 - 1.5 * IQR)
### Upper Bound: (Q3 + 1.5 * IQR)

Any data point less than the Lower Bound or more than the Upper Bound is considered as an outlier.

But the question was: Why only 1.5 times the IQR? Why not any other number?

Well, as you might have guessed, the number (here 1.5, hereinafter scale) clearly controls the sensitivity of the range and hence the decision rule. A bigger scale would make the outlier(s) to be considered as data point(s) while a smaller one would make some of the data point(s) to be perceived as outlier(s). And we’re quite sure, none of these cases is desirable.

But this is an abstract way of explaining the reason, it’s quite effective, but naive nonetheless. So to what should we turn our heads for hope?

Maths! Of course! (You saw that coming, right? 😐)

### You might be surprised if I tell you that this number, or scale, depends on the distribution followed by the data.
For example, let’s say our data follows, our beloved, Gaussian Distribution.

### Gaussian Distribution
You all must have seen how a Gaussian Distribution looks like, right? If not, here it is (although I’m suspicious about you 👊).

![image](https://user-images.githubusercontent.com/99672298/190670528-2c8e0f12-5efb-4b21-a2d7-2309d76a489b.png)

There are certain observations which could be inferred from this figure:

+ About 68.26% of the whole data lies within one standard deviation (<σ) of the mean (μ), taking both sides into account, the pink region in the figure.
+ About 95.44% of the whole data lies within two standard deviations (2σ) of the mean (μ), taking both sides into account, the pink+blue region in the figure.
+ About 99.72% of the whole data lies within three standard deviations (<3σ) of the mean (μ), taking both sides into account, the pink+blue+green region in the figure.
+ And the rest 0.28% of the whole data lies outside three standard deviations (>3σ) of the mean (μ), taking both sides into account, the little red region in the figure. And this part of the data is considered as outliers.
+ The first and the third quartiles, Q1 and Q3, lies at -0.675σ and +0.675σ from the mean, respectively.


### Let’s calculate the IQR decision range in terms of σ
#### Taking scale = 1:

    Lower Bound:
    = Q1 - 1 * IQR
    = Q1 - 1 * (Q3 - Q1)
    = -0.675σ - 1 * (0.675 - [-0.675])σ
    = -0.675σ - 1 * 1.35σ
    = -2.025σ
    Upper Bound:
    = Q3 + 1 * IQR
    = Q3 + 1 * (Q3 - Q1)
    = 0.675σ + 1 * (0.675 - [-0.675])σ
    = 0.675σ + 1 * 1.35σ
    = 2.025σ
   
So, when scale is taken as 1, then according to IQR Method any data which lies beyond 2.025σ from the mean (μ), on either side, shall be considered as outlier. But as we know, upto 3σ, on either side of the μ ,the data is useful. So we cannot take scale = 1, because this makes the decision range too exclusive, means this results in too much outliers. In other words, the decision range gets so small (compared to 3σ) that it considers some data points as outliers, which is not desirable.

### Taking scale = 2:

      Lower Bound:
      = Q1 - 2 * IQR
      = Q1 - 2 * (Q3 - Q1)
      = -0.675σ - 2 * (0.675 - [-0.675])σ
      = -0.675σ - 2 * 1.35σ
      = -3.375σ
      Upper Bound:
      = Q3 + 2 * IQR
      = Q3 + 2 * (Q3 - Q1)
      = 0.675σ + 2 * (0.675 - [-0.675])σ
      = 0.675σ + 2 * 1.35σ
      = 3.375σ
      
So, when scale is taken as 2, then according to IQR Method any data which lies beyond 3.375σ from the mean (μ), on either side, shall be considered as outlier. But as we know, upto 3σ, on either side of the μ ,the data is useful. So we cannot take scale = 2, because this makes the decision range too inclusive, means this results in too few outliers. In other words, the decision range gets so big (compared to 3σ) that it considers some outliers as data points, which is not desirable either.

### Taking scale = 1.5:

      Lower Bound:
      = Q1 - 1.5 * IQR
      = Q1 - 1.5 * (Q3 - Q1)
      = -0.675σ - 1.5 * (0.675 - [-0.675])σ
      = -0.675σ - 1.5 * 1.35σ
      = -2.7σ
      Upper Bound:
      = Q3 + 1.5 * IQR
      = Q3 + 1.5 * (Q3 - Q1)
      = 0.675σ + 1.5 * (0.675 - [-0.675])σ
      = 0.675σ + 1.5 * 1.35σ
      = 2.7σ
      
**When scale is taken as 1.5, then according to IQR Method any data which lies beyond 2.7σ from the mean (μ), on either side, shall be considered as outlier. And this decision range is the closest to what Gaussian Distribution tells us, i.e., 3σ. In other words, this makes the decision rule closest to what Gaussian Distribution considers for outlier detection, and this is exactly what we wanted.**

## Central Limit Theorem

Instead of analyzing entire population data, we always take out a sample for analysis. The problem with sampling is that “sample means is a random variable – varies for different samples”. And random sample we draw can never be an exact representation of the population. This phenomenon is called sample variation.

To nullify the sample variation, we use the central limit theorem. And according to the Central Limit Theorem:

1. The distribution of sample means follows a normal distribution if the population is normal.

2. the distribution of sample means follows a normal distribution even though the population is not normal. But the sample size should be large enough.

3. The grand average of all the sample mean values give us the population mean.

4. Theoretically, the sample size should be 30. And practically, the condition on the sample size (n) is:

n > 10(k3)2, where k3 is the sample skewness.

n > 10(k4), where K4 is the sample Kurtosis.

### What Is the Central Limit Theorem (CLT)?
In probability theory, the central limit theorem (CLT) states that the distribution of a sample variable approximates a normal distribution (i.e., a “bell curve”) as the sample size becomes larger, assuming that all samples are identical in size, and regardless of the population's actual distribution shape.

Put another way, CLT is a statistical premise that, given a sufficiently large sample size from a population with a finite level of variance, the mean of all sampled variables from the same population will be approximately equal to the mean of the whole population. Furthermore, these samples approximate a normal distribution, with their variances being approximately equal to the variance of the population as the sample size gets larger, according to the law of large numbers.

![image](https://user-images.githubusercontent.com/99672298/191051450-ea159eb3-9b6d-45e3-abe5-64a65e370cf1.png)

### Understanding the Central Limit Theorem (CLT)
According to the central limit theorem, the mean of a sample of data will be closer to the mean of the overall population in question, as the sample size increases, notwithstanding the actual distribution of the data. In other words, the data is accurate whether the distribution is normal or aberrant.

As a general rule, sample sizes of around 30-50 are deemed sufficient for the CLT to hold, meaning that the distribution of the sample means is fairly normally distributed. Therefore, the more samples one takes, the more the graphed results take the shape of a normal distribution. Note, however, that the central limit theorem will still be approximated in many cases for much smaller sample sizes, such as n=8 or n=5.
3

The central limit theorem is often used in conjunction with the law of large numbers, which states that the average of the sample means and standard deviations will come closer to equaling the population mean and standard deviation as the sample size grows, which is extremely useful in accurately predicting the characteristics of populations.

+ Sampling is random. All samples must be selected at random so that they have the same statistical possibility of being selected.
+ Samples should be independent. The selections or results from one sample should have no bearing on future samples or other sample results.
+ Samples should be limited. It's often cited that a sample should be no more than 10% of a population if sampling is done without replacement. In general, larger population sizes warrant the use of larger sample sizes.
+ Sample size is increasing. The central limit theorem is relevant as more samples are selected.

### The Central Limit Theorem in Finance
The CLT is useful when examining the returns of an individual stock or broader indices, because the analysis is simple, due to the relative ease of generating the necessary financial data. Consequently, investors of all types rely on the CLT to analyze stock returns, construct portfolios, and manage risk.

Say, for example, an investor wishes to analyze the overall return for a stock index that comprises 1,000 equities. In this scenario, that investor may simply study a random sample of stocks to cultivate estimated returns of the total index. To be safe, at least 30-50 randomly selected stocks across various sectors should be sampled for the central limit theorem to hold. Furthermore, previously selected stocks must be swapped out with different names to help eliminate bias.

### Why Is the Central Limit Theorem Useful?
The central limit theorem is useful when analyzing large data sets because it allows one to assume that the sampling distribution of the mean will be normally-distributed in most cases. This allows for easier statistical analysis and inference. For example, investors can use central limit theorem to aggregate individual security performance data and generate distribution of sample means that represent a larger population distribution for security returns over a period of time.

### Why Is the Central Limit Theorem's Minimize Sample Size 30?
A sample size of 30 is fairly common across statistics. A sample size of 30 often increases the confidence interval of your population data set enough to warrant assertions against your findings.
4
 The higher your sample size, the more likely the sample will be representative of your population set.

### What Is the Formula for Central Limit Theorem?
The central limit theorem doesn't have its own formula, but it relies on sample mean and standard deviation. As sample means are gathered from the population, standard deviation is used to distribute the data across a probability distribution curve.

## Sampling techniques

![maxresdefault](https://user-images.githubusercontent.com/99672298/190982038-48814cf2-36b0-4c0c-ba3d-5e482215b5ab.jpg)
![sampling-frame-l](https://user-images.githubusercontent.com/99672298/190982014-bf195c6b-bf2e-4e80-8984-ab791c30488c.jpg)
![13 09 2022_15 00 15_REC](https://user-images.githubusercontent.com/99672298/190981696-9dc82d2b-d7d0-43ce-884c-d22c4863798a.png)
![12 09 2022_20 35 15_REC](https://user-images.githubusercontent.com/99672298/190981714-719a460c-f90f-4569-ac11-17e17768276d.png)
![26 02 2022_13 24 51_REC](https://user-images.githubusercontent.com/99672298/190982694-60e2e38a-d455-4b9f-a901-06ce57ff0bb6.png)
![26 02 2022_13 25 16_REC](https://user-images.githubusercontent.com/99672298/190982664-fce582ac-42dd-45ec-9ce2-258dfd43ba4e.png)
![26 02 2022_13 25 30_REC](https://user-images.githubusercontent.com/99672298/190982651-35e81bed-667f-4b1f-a83d-1ef7bb6b9dd1.png)
![26 02 2022_13 25 44_REC](https://user-images.githubusercontent.com/99672298/190982636-8fa83fc2-4efe-4b7b-91fe-dbf0e4b45684.png)
![26 02 2022_13 25 59_REC](https://user-images.githubusercontent.com/99672298/190982621-15f87cde-6ecf-4f08-93fa-fafee27ab9e3.png)
![stratified-sampling-example-vector-illustration-diagram-research-method-explanation-scheme-person-symbols-stages-175044570](https://user-images.githubusercontent.com/99672298/190982306-e600f6a2-5761-46e3-9307-e851b201edeb.jpg)
![StratifiedRandomSampling](https://user-images.githubusercontent.com/99672298/190982472-9f346a6d-afab-40bd-815f-7f37b5fae9be.jpg)
![stratified-random-sampling-2-768x460](https://user-images.githubusercontent.com/99672298/190982325-d585230e-b005-4e92-bfea-09c5c04b8e76.jpg)
![stratified-cluster-sampling-l](https://user-images.githubusercontent.com/99672298/190982455-424f3441-91d1-423e-89f2-18a9af2824a5.jpg)
![images](https://user-images.githubusercontent.com/99672298/190982464-68d994c1-b52c-48de-abc8-eff35923a708.png)

![TC_606106-heterogeneous-and-homogeneous-mixtures1-5ac4f1a9642dca0036847e52](https://user-images.githubusercontent.com/99672298/190981958-c427f4bc-ced1-4f25-a483-581fed6fe83c.png)

## Sampling Error

![14 09 2022_15 26 06_REC](https://user-images.githubusercontent.com/99672298/190981654-40f11b96-e4e7-458e-95bc-bc0cd55ec263.png)
![14 09 2022_13 01 17_REC](https://user-images.githubusercontent.com/99672298/190981679-575129e1-463f-4707-8be4-d51fd1369780.png)

## Normal distribution

![16 09 2022_16 31 55_REC](https://user-images.githubusercontent.com/99672298/190981214-f7ef6fda-825d-43e1-9e8c-70c5d2f5fa0f.png)
![16 09 2022_18 56 19_REC](https://user-images.githubusercontent.com/99672298/190981163-f92e9d08-9ab8-463e-bd62-7a34558b3530.png)
![16 09 2022_18 55 14_REC](https://user-images.githubusercontent.com/99672298/190981186-296b8ac1-1fb8-4d19-8d75-fe04120328ae.png)
![common-distribution_huec055ab3bd48482f92384431f8f73d3e_75898_900x500_fit_box_2](https://user-images.githubusercontent.com/99672298/190981750-bf877d2f-816a-49bc-bc43-371d54b9ec40.png)

## Properties of Normal Distribution

![16 09 2022_19 52 31_REC](https://user-images.githubusercontent.com/99672298/190980913-dc1a9cf4-9469-4ae7-83fe-15f0f366f263.png)

## Outliers

![24 03 2022_11 04 54_REC](https://user-images.githubusercontent.com/99672298/190962879-0b5cda86-cd05-4638-8be0-9ee64f732117.png)

## QQ- Plot

## Probability Density function
## Feature scaling Transformations
## log transformation
## square- root transformation
## reciprocal transformation
## Box Cox Transformation
## Yeo-johnson transformation
## Euclidean Distance
## Manhattan distance
## Minkowski Distance
## Covariance

![07 05 2022_13 13 53_REC](https://user-images.githubusercontent.com/99672298/190981837-3684c83b-3a03-494a-b1e2-c4cec3dbd611.png)
![04 05 2022_20 05 57_REC](https://user-images.githubusercontent.com/99672298/190981878-f7ef77f3-6ea8-49b1-97c9-909c5baf0ada.png)
![Covariance-Formula](https://user-images.githubusercontent.com/99672298/190981973-c91ce53d-c933-48e7-bab1-8ef22905c33f.jpg)


## Correlation

![31 03 2022_12 27 48_REC](https://user-images.githubusercontent.com/99672298/190981901-f8fd0acf-458d-47d1-ba48-ebcaef9abf37.png)
