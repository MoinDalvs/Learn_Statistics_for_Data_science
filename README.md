# Learn_basic_stats_for-Data_science

![inbox_221701_c1e7f0c6c305bffe3d62bd37d8783ed8_prob](https://user-images.githubusercontent.com/99672298/190671924-4c9b0dd4-243b-4828-9ade-f91a0555bb16.png)
![30 03 2022_15 52 11_REC](https://user-images.githubusercontent.com/99672298/190672178-6591a16f-0939-4262-9226-3bcfb40a7e7f.png)
![30 03 2022_15 52 32_REC](https://user-images.githubusercontent.com/99672298/190672197-76e232bd-1202-408c-a7b0-1bde68671d38.png)
![30 03 2022_15 53 01_REC](https://user-images.githubusercontent.com/99672298/190672213-c2e51ed4-1840-4fa5-b51e-6a4916968f4f.png)

## Mean
## MEdian
## Mode
## Central Limit Theorem
## Standard and Normal Distribution
## Boxplot IQR 
## Outliers
## Variance
## Standard Daviation
## Skewness
## Central Limit theorem
## QQ- Plot
## Normal distribution
## Properties of Normal Distribution
## Probability Density function
## Feature scaling Transformations
## log transformation
## square- root transformation
## reciprocal transformation
## Box Cox Transformation
## Yeo-johnson transformation
## Euclidean Distance
## Manhattan distance
## Minkowski Distance
## Covariance
## Correlation

![sk1](https://user-images.githubusercontent.com/99672298/190672365-7fcd7401-b263-4a2e-916b-352663be040c.png)
![FRBfMkKaIAAjrd2](https://user-images.githubusercontent.com/99672298/190672080-c7c560e4-5447-4743-970d-839ee85898a0.png)

## Kurtosis

Kurtosis is a measure of the tailedness of a distribution. Tailedness is how often outliers occur. Excess kurtosis is the tailedness of a distribution relative to a normal distribution.
Like skewness, kurtosis is a statistical measure that is used to describe distribution. Whereas skewness differentiates extreme values in one versus the other tail, kurtosis measures extreme values in either tail. Distributions with large kurtosis exhibit tail data exceeding the tails of the normal distribution (e.g., five or more standard deviations from the mean). Distributions with low kurtosis exhibit tail data that are generally less extreme than the tails of the normal distribution.

For investors, high kurtosis of the return distribution implies the investor will experience occasional extreme returns (either positive or negative), more extreme than the usual + or - three standard deviations from the mean that is predicted by the normal distribution of returns. This phenomenon is known as kurtosis risk.

![image](https://user-images.githubusercontent.com/99672298/190677717-f516eb9b-caed-4d49-95c4-60f6ea71290c.png)

Distributions with medium kurtosis (medium tails) are mesokurtic.
Distributions with low kurtosis (thin tails) are platykurtic.
Distributions with high kurtosis (fat tails) are leptokurtic.
Tails are the tapering ends on either side of a distribution. They represent the probability or frequency of values that are extremely high or low compared to the mean. In other words, tails represent how often outliers occur.

Example: Types of kurtosis

![image](https://user-images.githubusercontent.com/99672298/190672762-187cba1e-f8ed-4e98-a851-13b3af6f6f00.png)

Types of kurtosis
Distributions can be categorized into three groups based on their kurtosis:

![image](https://user-images.githubusercontent.com/99672298/190672814-2c8e38aa-38ef-459f-9158-3787b61143a5.png)
![image](https://user-images.githubusercontent.com/99672298/190672949-b7dca686-02f5-4037-b1bc-f4cb61f76a46.png)
![image](https://user-images.githubusercontent.com/99672298/190673076-13f3345f-f863-44d1-8d32-27fc5da83be1.png)

From the graph, we can see that the frequency distribution (shown by the gray bars) approximately follows a normal distribution (shown by the green curve). Normal distributions are mesokurtic.

The zoologist calculates the kurtosis of the sample. She finds that the kurtosis is 3.09 and the excess kurtosis is 0.09, and she concludes that the distribution is mesokurtic.

![image](https://user-images.githubusercontent.com/99672298/190673209-a780b0f0-f8c4-4988-a69c-0b202716988b.png)
![image](https://user-images.githubusercontent.com/99672298/190677433-37b49df9-0423-44cd-b809-77456ecdd5fe.png)
![image](https://user-images.githubusercontent.com/99672298/190677498-ad68ddd8-61f2-4373-9250-800544e91b3b.png)

## The Difference Between Standard Deviation and Average Deviation

### Standard Deviation Versus Average Deviation
Two of the most popular ways to measure variability or volatility in a set of data are standard deviation and average deviation, also known as mean absolute deviation. Though the two measurements are similar, they are calculated differently and offer slightly different views of data.

Determining volatility‚Äîthat is, deviation from the center‚Äîis important in finance, so professionals in accounting, investing, and economics should be familiar with both concepts.

#### KEY TAKEAWAYS
+ Standard deviation is the most common measure of variability and is frequently used to determine the volatility of financial instruments and investment returns.
+ Standard deviation is considered the most appropriate measure of variability when using a population sample, when the mean is the best measure of center, and when the distribution of data is normal.
+ Some argue that average deviation, or mean absolute deviation, is a better gauge of variability when there are distant outliers or the data is not well distributed.

### Understanding Standard Deviation
Standard deviation is the most common measure of variability and is frequently used to determine the volatility of markets, financial instruments, and investment returns. To calculate the standard deviation:

+ Find the mean, or average, of the data points by adding them and dividing the total by the number of data points.
+ Subtract the mean from each data point and square the difference of each result.
+ Find the mean of those squared differences and then the square root of the mean.

Squaring the differences between each point and the mean avoids the issue of negative differences for values below the mean, but it means the variance is no longer in the same unit of measure as the original data. Taking the square root means the standard deviation returns to the original unit of measure and is easier to interpret and use in further calculations.

### Average Deviation
The average deviation, or mean absolute deviation, is calculated similarly to standard deviation, but it uses absolute values instead of squares to circumvent the issue of negative differences between the data points and their means.

To calculate the average deviation:

+ Calculate the mean of all data points.
+ Calculate the difference between the mean and each data point.
+ Calculate the average of the absolute values of those differences.

### Standard Deviation Versus Average Deviation
Standard deviation is often used to measure the volatility of returns from investment funds or strategies because it can help measure volatility. Higher volatility is generally associated with a higher risk of losses, so investors want to see higher returns from funds that generate higher volatility. For example, a stock index fund should have relatively low standard deviation compared with a growth fund.

The mean average, or mean absolute deviation, is considered the closest alternative to standard deviation. It is also used to gauge volatility in markets and financial instruments, but it is used less frequently than standard deviation.

According to mathematicians, when a data set is of normal distribution‚Äîthat is, there aren't many outliers‚Äîstandard deviation is generally the preferable gauge of variability. But when there are large outliers, standard deviation registers higher levels of dispersion (or deviation from the center) than mean absolute deviation.

## Percentile vs. Quartile vs. Quantile: What‚Äôs the Difference?

Three terms that students often confuse in statistics are percentiles, quartiles, and quantiles.

Here‚Äôs a simple definition of each:

Percentiles: Range from 0 to 100.

Quartiles: Range from 0 to 4.

+ Note that percentiles and quartiles share the following relationship:

+ 0 percentile = 0 quartile (also called the minimum)
+ 25th percentile = 1st quartile
+ 50th percentile = 2nd quartile (also called the median)
+ 75th percentile = 3rd quartile
+ 100th percentile = 4th quartile (also called the maximum)

Quantiles: Range from any value to any other value.

Note that percentiles and quartiles are simply types of quantiles.

![image](https://user-images.githubusercontent.com/99672298/190623416-d0621d79-121d-4ff2-8a98-083a2ec825ae.png)

Some types of quantiles even have specific names, including:

+ 4-quantiles are called quartiles.
+ 5-quantiles are called quintiles.
+ 8-quantiles are called octiles.
+ 10-quantiles are called deciles.
+ 100-quantiles are called percentiles.

### What is a Quantile?
The word ‚Äúquantile‚Äù comes from the word quantity. In simple terms, a quantile is where a sample is divided into equal-sized, adjacent, subgroups (that‚Äôs why it‚Äôs sometimes called a ‚Äúfractile‚Äú). It can also refer to dividing a probability distribution into areas of equal probability.

The median is a quantile; the median is placed in a probability distribution so that exactly half of the data is lower than the median and half of the data is above the median. The median cuts a distribution into two equal areas and so it is sometimes called 2-quantile.

Quartiles are also quantiles; they divide the distribution into four equal parts. Percentiles are quantiles that divide a distribution into 100 equal parts and deciles are quantiles that divide a distribution into 10 equal parts

## ‚ÄúWhy 1.5 times IQR? Why not 1 or 2 or any other number?‚Äù

In the most general sense, an outlier is a data point which differs significantly from other observations.
### IQR Method of Outlier Detection
To explain IQR Method easily, let‚Äôs start with a box plot.

![image](https://user-images.githubusercontent.com/99672298/190669815-0ba7ed2e-9282-47c4-98bf-84c0600d0753.png)

A box plot tells us, more or less, about the distribution of the data. It gives a sense of how much the data is actually spread about, what‚Äôs its range, and about its skewness. As you might have noticed in the figure, that a box plot enables us to draw inference from it for an ordered data, i.e., it tells us about the various metrics of a data arranged in ascending order.

In the above figure,

+ minimum is the minimum value in the dataset,
+ and maximum is the maximum value in the dataset.

So the difference between the two tells us about the range of dataset.

+ The median is the median (or centre point), also called second quartile, of the data (resulting from the fact that the data is ordered).
+ Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.
+ Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3.
+ The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR.
### IQR = Q3 - Q1

To detect the outliers using this method, we define a new range, let‚Äôs call it decision range, and any data point lying outside this range is considered as outlier and is accordingly dealt with. The range is as given below:

### Lower Bound: (Q1 - 1.5 * IQR)
### Upper Bound: (Q3 + 1.5 * IQR)

Any data point less than the Lower Bound or more than the Upper Bound is considered as an outlier.

But the question was: Why only 1.5 times the IQR? Why not any other number?

Well, as you might have guessed, the number (here 1.5, hereinafter scale) clearly controls the sensitivity of the range and hence the decision rule. A bigger scale would make the outlier(s) to be considered as data point(s) while a smaller one would make some of the data point(s) to be perceived as outlier(s). And we‚Äôre quite sure, none of these cases is desirable.

But this is an abstract way of explaining the reason, it‚Äôs quite effective, but naive nonetheless. So to what should we turn our heads for hope?

Maths! Of course! (You saw that coming, right? üòê)

### You might be surprised if I tell you that this number, or scale, depends on the distribution followed by the data.
For example, let‚Äôs say our data follows, our beloved, Gaussian Distribution.

### Gaussian Distribution
You all must have seen how a Gaussian Distribution looks like, right? If not, here it is (although I‚Äôm suspicious about you üëä).

![image](https://user-images.githubusercontent.com/99672298/190670528-2c8e0f12-5efb-4b21-a2d7-2309d76a489b.png)

There are certain observations which could be inferred from this figure:

+ About 68.26% of the whole data lies within one standard deviation (<œÉ) of the mean (Œº), taking both sides into account, the pink region in the figure.
+ About 95.44% of the whole data lies within two standard deviations (2œÉ) of the mean (Œº), taking both sides into account, the pink+blue region in the figure.
+ About 99.72% of the whole data lies within three standard deviations (<3œÉ) of the mean (Œº), taking both sides into account, the pink+blue+green region in the figure.
+ And the rest 0.28% of the whole data lies outside three standard deviations (>3œÉ) of the mean (Œº), taking both sides into account, the little red region in the figure. And this part of the data is considered as outliers.
+ The first and the third quartiles, Q1 and Q3, lies at -0.675œÉ and +0.675œÉ from the mean, respectively.

### Let‚Äôs calculate the IQR decision range in terms of œÉ
#### Taking scale = 1:

    Lower Bound:
    = Q1 - 1 * IQR
    = Q1 - 1 * (Q3 - Q1)
    = -0.675œÉ - 1 * (0.675 - [-0.675])œÉ
    = -0.675œÉ - 1 * 1.35œÉ
    = -2.025œÉ
    Upper Bound:
    = Q3 + 1 * IQR
    = Q3 + 1 * (Q3 - Q1)
    = 0.675œÉ + 1 * (0.675 - [-0.675])œÉ
    = 0.675œÉ + 1 * 1.35œÉ
    = 2.025œÉ
   
So, when scale is taken as 1, then according to IQR Method any data which lies beyond 2.025œÉ from the mean (Œº), on either side, shall be considered as outlier. But as we know, upto 3œÉ, on either side of the Œº ,the data is useful. So we cannot take scale = 1, because this makes the decision range too exclusive, means this results in too much outliers. In other words, the decision range gets so small (compared to 3œÉ) that it considers some data points as outliers, which is not desirable.

### Taking scale = 2:

      Lower Bound:
      = Q1 - 2 * IQR
      = Q1 - 2 * (Q3 - Q1)
      = -0.675œÉ - 2 * (0.675 - [-0.675])œÉ
      = -0.675œÉ - 2 * 1.35œÉ
      = -3.375œÉ
      Upper Bound:
      = Q3 + 2 * IQR
      = Q3 + 2 * (Q3 - Q1)
      = 0.675œÉ + 2 * (0.675 - [-0.675])œÉ
      = 0.675œÉ + 2 * 1.35œÉ
      = 3.375œÉ
      
So, when scale is taken as 2, then according to IQR Method any data which lies beyond 3.375œÉ from the mean (Œº), on either side, shall be considered as outlier. But as we know, upto 3œÉ, on either side of the Œº ,the data is useful. So we cannot take scale = 2, because this makes the decision range too inclusive, means this results in too few outliers. In other words, the decision range gets so big (compared to 3œÉ) that it considers some outliers as data points, which is not desirable either.

### Taking scale = 1.5:

      Lower Bound:
      = Q1 - 1.5 * IQR
      = Q1 - 1.5 * (Q3 - Q1)
      = -0.675œÉ - 1.5 * (0.675 - [-0.675])œÉ
      = -0.675œÉ - 1.5 * 1.35œÉ
      = -2.7œÉ
      Upper Bound:
      = Q3 + 1.5 * IQR
      = Q3 + 1.5 * (Q3 - Q1)
      = 0.675œÉ + 1.5 * (0.675 - [-0.675])œÉ
      = 0.675œÉ + 1.5 * 1.35œÉ
      = 2.7œÉ
      
**When scale is taken as 1.5, then according to IQR Method any data which lies beyond 2.7œÉ from the mean (Œº), on either side, shall be considered as outlier. And this decision range is the closest to what Gaussian Distribution tells us, i.e., 3œÉ. In other words, this makes the decision rule closest to what Gaussian Distribution considers for outlier detection, and this is exactly what we wanted.**
